{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdcff611",
   "metadata": {},
   "source": [
    "## NLTK Practice\n",
    "- 1. Installing and Importing NLTK\n",
    "- 2. Tokenization\n",
    "---\n",
    "### 1. Installing and Importing NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09925da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac009017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74311dc8",
   "metadata": {},
   "source": [
    "### 2. Tokenization\n",
    "- Tokenization is the process of breaking down a text (like a sentence or paragraph) into smaller pieces called tokens. \n",
    "- It’s the first step in most NLP tasks (like translation, sentiment analysis, text classification).\n",
    "\n",
    "#### 2.1 Turning CORPUS into DOCUMENTS\n",
    "\n",
    "- If you have a corpus as a big chunk of text, you might want to split it into smaller pieces (the documents) so you can process or analyze them individually.\n",
    "    - CORPUS ---> Paragraph \n",
    "    - DOCUMENTS ---> Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3ef4f850",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"Hello welcome to my NLTK Prctice i.e., my rough work on nltk.\n",
    "Let's explore what nltk can do.\n",
    "I'm really excited! ready set go.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8f7a7de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello welcome to my NLTK Prctice i.e., my rough work on nltk.\n",
      "Let's explore what nltk can do.\n",
      "I'm really excited! ready set go.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b4afe162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "# nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dc0c3468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello welcome to my NLTK Prctice i.e., my rough work on nltk.',\n",
       " \"Let's explore what nltk can do.\",\n",
       " \"I'm really excited!\",\n",
       " 'ready set go.']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = sent_tokenize(corpus)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e5459c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello welcome to my NLTK Prctice i.e., my rough work on nltk.\n",
      "Let's explore what nltk can do.\n",
      "I'm really excited!\n",
      "ready set go.\n"
     ]
    }
   ],
   "source": [
    "for sent in documents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5602b283",
   "metadata": {},
   "source": [
    "#### 2.2 Turning DOCUMENTS into WORDS\n",
    "- Each document is a chunk of text, and word tokenization splits that text into individual words (tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "503426a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cca467ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " 'to',\n",
       " 'my',\n",
       " 'NLTK',\n",
       " 'Prctice',\n",
       " 'i.e.',\n",
       " ',',\n",
       " 'my',\n",
       " 'rough',\n",
       " 'work',\n",
       " 'on',\n",
       " 'nltk',\n",
       " '.',\n",
       " 'Let',\n",
       " \"'s\",\n",
       " 'explore',\n",
       " 'what',\n",
       " 'nltk',\n",
       " 'can',\n",
       " 'do',\n",
       " '.',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'really',\n",
       " 'excited',\n",
       " '!',\n",
       " 'ready',\n",
       " 'set',\n",
       " 'go',\n",
       " '.']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ccea6e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'welcome', 'to', 'my', 'NLTK', 'Prctice', 'i.e.', ',', 'my', 'rough', 'work', 'on', 'nltk', '.']\n",
      "['Let', \"'s\", 'explore', 'what', 'nltk', 'can', 'do', '.']\n",
      "['I', \"'m\", 'really', 'excited', '!']\n",
      "['ready', 'set', 'go', '.']\n"
     ]
    }
   ],
   "source": [
    "for sent in documents:\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "44dc3221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " 'to',\n",
       " 'my',\n",
       " 'NLTK',\n",
       " 'Prctice',\n",
       " 'i',\n",
       " '.',\n",
       " 'e',\n",
       " '.,',\n",
       " 'my',\n",
       " 'rough',\n",
       " 'work',\n",
       " 'on',\n",
       " 'nltk',\n",
       " '.',\n",
       " 'Let',\n",
       " \"'\",\n",
       " 's',\n",
       " 'explore',\n",
       " 'what',\n",
       " 'nltk',\n",
       " 'can',\n",
       " 'do',\n",
       " '.',\n",
       " 'I',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'really',\n",
       " 'excited',\n",
       " '!',\n",
       " 'ready',\n",
       " 'set',\n",
       " 'go',\n",
       " '.']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "wordpunct_tokenize(corpus) # will consider puncuations as words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8acc1485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " 'to',\n",
       " 'my',\n",
       " 'NLTK',\n",
       " 'Prctice',\n",
       " 'i.e.',\n",
       " ',',\n",
       " 'my',\n",
       " 'rough',\n",
       " 'work',\n",
       " 'on',\n",
       " 'nltk.',\n",
       " 'Let',\n",
       " \"'s\",\n",
       " 'explore',\n",
       " 'what',\n",
       " 'nltk',\n",
       " 'can',\n",
       " 'do.',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'really',\n",
       " 'excited',\n",
       " '!',\n",
       " 'ready',\n",
       " 'set',\n",
       " 'go',\n",
       " '.']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "tokenizer.tokenize(corpus)   # won't treat fullstop as a word will consider it in the previous word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8bc6c3",
   "metadata": {},
   "source": [
    "### 3. Stemming\n",
    "- Stemming is the process of reducing a word to its root word called **Stem**, that affixes, suffixes or perfixes to the root word known as a **Lemma**\n",
    "- Stemming is important in Natural Language Understanding (NLU) and Natural Langugae Processing\n",
    "- Stemming Examples\n",
    "    - [eat, eating, eaten] --> eat (root word, stem word)\n",
    "    - [running, run, ran] --> run  (root word, stem word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ec88af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['playing', 'played', 'plays', 'flying', 'flies', 'cried', 'crying', 'happier', 'happyly', 'studies', 'studying']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a54999c",
   "metadata": {},
   "source": [
    "#### 3.1 Porter Stemming\n",
    "- The Porter Stemmer is a widely used algorithm in natural language processing (NLP) for word stemming—reducing words to their base or root form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fb66e043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0f74f60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing ---> play\n",
      "played ---> play\n",
      "plays ---> play\n",
      "flying ---> fli\n",
      "flies ---> fli\n",
      "cried ---> cri\n",
      "crying ---> cri\n",
      "happier ---> happier\n",
      "happyly ---> happyli\n",
      "studies ---> studi\n",
      "studying ---> studi\n"
     ]
    }
   ],
   "source": [
    "for word in words :\n",
    "    print(f\"{word} ---> {stemming.stem(word)}\")\n",
    "# will give some errors e.g. [flying ---> fli], [crying ---> cri] etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e73d8ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('Congratulations') \n",
    "# returns word 'congratul' which completly changes the meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d6270218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sit\n",
      "ssit\n"
     ]
    }
   ],
   "source": [
    "print(stemming.stem('sitting')) # returns sit\n",
    "print(stemming.stem('ssitting')) # returns ssit\n",
    "\n",
    "# This problem will get fixed with the help of Lemmatzation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9278e99",
   "metadata": {},
   "source": [
    "#### 3.2 RegexpStemmer\n",
    "- The RegexpStemmer (Regular Expression Stemmer) is a simple and customizable rule-based stemmer that removes suffixes from words using regular expressions.\n",
    "- Unlike more complex stemmers like the PorterStemmer, which use rule sets and conditions, RegexpStemmer works by applying your specified regular expression—which makes it very flexible but also very manual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "720a7cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "reg_stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "21b472e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "ingeat\n"
     ]
    }
   ],
   "source": [
    "print(reg_stemmer.stem('eating'))\n",
    "print(reg_stemmer.stem('ingeating')) # returns 'ingeat' coz we addded '$' at last if we'll remove '$' then it will return 'eat' for the same input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0d11500c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n",
      "played\n",
      "play\n",
      "fly\n",
      "flie\n",
      "cried\n",
      "cry\n",
      "happier\n",
      "happyly\n",
      "studie\n",
      "study\n"
     ]
    }
   ],
   "source": [
    "for word in words :\n",
    "    print(reg_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da3d05",
   "metadata": {},
   "source": [
    "#### 3.3 Snowball Stemmer\n",
    "-  The Snowball Stemmer uses an improved version of the original Porter algorithm (often called Porter2), which is less aggressive and more accurate.\n",
    "-  Unlike the Porter Stemmer, which primarily works for English, the Snowball Stemmer supports several languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c6f18d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "snowballstemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c841b781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing ---> play\n",
      "played ---> play\n",
      "plays ---> play\n",
      "flying ---> fli\n",
      "flies ---> fli\n",
      "cried ---> cri\n",
      "crying ---> cri\n",
      "happier ---> happier\n",
      "happyly ---> happyli\n",
      "studies ---> studi\n",
      "studying ---> studi\n"
     ]
    }
   ],
   "source": [
    "for word in words :\n",
    "    print(f'{word} ---> {snowballstemmer.stem(word)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3800b14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter : fairli sportingli\n",
      "Snowball : fair sport\n"
     ]
    }
   ],
   "source": [
    "print('Porter : ' + stemming.stem('fairly'), stemming.stem('sportingly'))\n",
    "\n",
    "print('Snowball : ' + snowballstemmer.stem('fairly'), snowballstemmer.stem('sportingly'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb564fa",
   "metadata": {},
   "source": [
    "### 4. Lemmatization\n",
    "\n",
    "- Lemmatization is another text preprocessing technique in Natural Language Processing (NLP) that aims to reduce words to their base or dictionary form (called a lemma).\n",
    "- Unlike stemming, which often removes suffixes in a mechanical or rule-based manner, lemmatization takes into account the context and the part of speech of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "994a7cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e203d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vaibh\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d41085",
   "metadata": {},
   "source": [
    "#### 4.1 The Part Of Speech tag. Valid options are :\n",
    "- \"n\" : nouns **(By Deafult)**\n",
    "- \"v\" : verbs\n",
    "- \"a\" : adjectives \n",
    "- \"r\" : adverbs \n",
    "- \"s\" : satellite adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "47bba0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('going', pos = 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b1096e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "27f52682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing ---> play\n",
      "played ---> play\n",
      "plays ---> play\n",
      "flying ---> fly\n",
      "flies ---> fly\n",
      "cried ---> cry\n",
      "crying ---> cry\n",
      "happier ---> happier\n",
      "happyly ---> happyly\n",
      "studies ---> study\n",
      "studying ---> study\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(f'{word} ---> {lemmatizer.lemmatize(word, pos = 'v')}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45f0791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83ad600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
